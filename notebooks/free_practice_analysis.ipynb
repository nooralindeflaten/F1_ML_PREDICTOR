{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1\n",
    "from fastf1 import get_session\n",
    "from fastf1.utils import delta_time\n",
    "import matplotlib.pyplot as plt\n",
    "# Features and target\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaded all season data, quali and race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastf1.Cache.enable_cache('/Users/nooralindeflaten/f1_ML_predictor/data/cache')\n",
    "session = get_session(2023, 'Australian GP', 'Q')\n",
    "session.load()\n",
    "track = session.get_circuit_info()\n",
    "track.corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_circuit_properties(circuit_info):\n",
    "    print(\"Circuit Properties:\")\n",
    "    for prop in dir(circuit_info):\n",
    "        if not prop.startswith('_'):\n",
    "            print(prop)\n",
    "    print(\"\\n\")\n",
    "    print(\"Circuit Methods:\")\n",
    "    for method in dir(circuit_info):\n",
    "        if not method.startswith('_'):\n",
    "            method_obj = getattr(circuit_info, method)\n",
    "            print(f\"{method} -> {type(method_obj)}\")\n",
    "            if callable(method_obj):\n",
    "                print(f\"{method}()\")      \n",
    "\n",
    "track.corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastf1.Cache.enable_cache('/Users/nooralindeflaten/f1_ML_predictor/data/cache')\n",
    "\n",
    "\n",
    "practice_sessions = ['FP1','FP2','FP3']\n",
    "\n",
    "all_laps_by_session = {}\n",
    "\n",
    "for session_type in practice_sessions:\n",
    "    try:\n",
    "        session = fastf1.get_session(2022,7,session_type)\n",
    "        session.load()\n",
    "        laps = session.laps[session.laps['Driver'] == 'LEC']  # or all drivers if you remove this\n",
    "        all_laps_by_session[session_type] = laps  # This is a FastF1 `Laps` object\n",
    "    except Exception as e:\n",
    "        print(f'Failed {e}')\n",
    "\n",
    "races = fastf1.get_session(2022,7,'R')\n",
    "races.load()\n",
    "leclerc_race_laps = races.laps[races.laps['Driver'] == 'LEC']\n",
    "\n",
    "quali = fastf1.get_session(2022,7,'Q')\n",
    "quali.load()\n",
    "leclerc_quali_laps = quali.laps[quali.laps['Driver'] == 'LEC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_degradation_data(laps,weather_df):\n",
    "    deg_data = []\n",
    "\n",
    "    for _, lap in laps.iterlaps():\n",
    "        try:\n",
    "            car_data = lap.get_car_data()\n",
    "            speed_avg = car_data['Speed'].mean()\n",
    "            tyrelife = lap['TyreLife']\n",
    "            compound = lap['Compound']\n",
    "            laptime = lap['LapTime'].total_seconds()\n",
    "            lap_time = lap['LapStartTime'].to_pytimedelta()\n",
    "\n",
    "            # Find closest weather timestamp\n",
    "            weather_row = weather_df.iloc[(weather_df['Time'] - lap_time).abs().argmin()]\n",
    "            track_temp = weather_row['TrackTemp']\n",
    "            pressure = weather_row['Pressure']\n",
    "            air_temp = weather_row['AirTemp']\n",
    "\n",
    "            deg_data.append({\n",
    "                'lap_number': lap['LapNumber'],\n",
    "                'compound': compound,\n",
    "                'tyrelife': tyrelife,\n",
    "                'laptime': laptime,\n",
    "                'speed_avg': speed_avg,\n",
    "                'track_temp': track_temp,\n",
    "                'pressure': pressure,\n",
    "                'air_temp': air_temp\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping lap due to error: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(deg_data)\n",
    "\n",
    "print(get_degradation_data(fp1,fp1.get_weather_data()))\n",
    "def all_weather(all_laps_by_session):\n",
    "    for sess in all_laps_by_session:\n",
    "        weather_df = all_laps_by_session[sess].get_weather_data()\n",
    "        laps = all_laps_by_session[sess]\n",
    "        weather_fp1 = get_degradation_data(laps,weather_df)\n",
    "        sns.lmplot(data=weather_fp1, x='tyrelife', y='speed_avg', hue='compound')\n",
    "        plt.title(\"Tyre Degradation â€“ Avg Speed vs Tyre Life\")\n",
    "\n",
    "all_weather(all_laps_by_session=all_laps_by_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def build_practice_df_d(lap_summary_df,practice):\n",
    "    # Step 1: Parse LapStartTime to align with car data timestamps\n",
    "    lap_summary_df['LapStartTime'] = pd.to_timedelta(lap_summary_df['LapStartTime'])\n",
    "\n",
    "    # Step 2: Convert car_data time to timedelta\n",
    "    # Step 3: Group car data into laps\n",
    "    lap_aggregates = []\n",
    "\n",
    "    for _, lap in lap_summary_df.iterlaps():\n",
    "        try:\n",
    "            car_data = lap.get_car_data().add_distance()  # You can drop .add_distance() if not needed\n",
    "\n",
    "            avg_speed = car_data['Speed'].mean()\n",
    "            avg_rpm = car_data['RPM'].mean()\n",
    "            avg_throttle = car_data['Throttle'].mean()\n",
    "            avg_brake = car_data['Brake'].mean()\n",
    "            weather_data = lap.get_weather_data()\n",
    "\n",
    "            lap_aggregates.append({\n",
    "                str(practice): practice,\n",
    "                'LapNumber': lap['LapNumber'],\n",
    "                'speed_avg': avg_speed,\n",
    "                'rpm_avg': avg_rpm,\n",
    "                'throttle_avg': avg_throttle,\n",
    "                'brake_avg': avg_brake,\n",
    "                'TrackTemp': weather_data['TrackTemp'],\n",
    "                'Pressure': weather_data['Pressure'],\n",
    "                'AirTemp': weather_data['AirTemp'],\n",
    "                'Humidity': weather_data['Humidity'],\n",
    "                'WindSpeed': weather_data['WindSpeed'],\n",
    "                'Rainfall': weather_data['Rainfall'],\n",
    "                'WindDirection': weather_data['WindDirection']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping lap {lap.LapNumber} due to error: {e}\")\n",
    "\n",
    "    agg_df = pd.DataFrame(lap_aggregates)\n",
    "\n",
    "    # Step 4: Merge aggregates with lap_summary\n",
    "    combined_df = lap_summary_df.merge(agg_df, on='LapNumber', how='left')\n",
    "\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "weather = {\n",
    "    'track_temp': 54.7,\n",
    "    'pressure': 1015.2,\n",
    "    'humidity': 70.0  # Optional: add what you have\n",
    "}\n",
    "\n",
    "# Assuming your raw dataframes are named:\n",
    "# leclerc_laps, leclerc_car_data\n",
    "practice_df = build_practice_df_d(fp1,'FP1')\n",
    "\n",
    "practice_df['FP1']  # View the output!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building practice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fp1 = all_laps_by_session['FP1']\n",
    "fp2 = all_laps_by_session['FP2']\n",
    "fp3 = all_laps_by_session['FP3']\n",
    "\n",
    "\n",
    "def build_practice_df(lap_summary_df, practice_name):\n",
    "    lap_summary_df['LapStartTime'] = pd.to_timedelta(lap_summary_df['LapStartTime'])\n",
    "\n",
    "    lap_aggregates = []\n",
    "\n",
    "    for _, lap in lap_summary_df.iterlaps():\n",
    "        try:\n",
    "            car_data = lap.get_car_data().add_distance()\n",
    "\n",
    "            lap_aggregates.append({\n",
    "                'LapNumber': lap['LapNumber'],\n",
    "                'speed_avg': car_data['Speed'].mean(),\n",
    "                'rpm_avg': car_data['RPM'].mean(),\n",
    "                'throttle_avg': car_data['Throttle'].mean(),\n",
    "                'brake_avg': car_data['Brake'].mean(),\n",
    "                'TrackTemp': lap.get_weather_data()['TrackTemp'],\n",
    "                'Pressure': lap.get_weather_data()['Pressure'],\n",
    "                'AirTemp': lap.get_weather_data()['AirTemp'],\n",
    "                'Humidity': lap.get_weather_data()['Humidity'],\n",
    "                'WindSpeed': lap.get_weather_data()['WindSpeed'],\n",
    "                'Rainfall': lap.get_weather_data()['Rainfall'],\n",
    "                'WindDirection': lap.get_weather_data()['WindDirection'],\n",
    "                'Session': practice_name  # ðŸ†• Add session tag here\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping lap {lap.LapNumber} due to error: {e}\")\n",
    "\n",
    "    agg_df = pd.DataFrame(lap_aggregates)\n",
    "\n",
    "    combined_df = lap_summary_df.merge(agg_df, on='LapNumber', how='left')\n",
    "    return combined_df\n",
    "\n",
    "fp1 = build_practice_df(fp1, \"FP1\")\n",
    "fp2 = build_practice_df(fp2, \"FP2\")\n",
    "fp3 = build_practice_df(fp3, \"FP3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading practice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp1 = pd.read_csv(\"/Users/nooralindeflaten/f1_ML_predictor/data/processed/leclerc_2022_monaco_fp1.csv\")\n",
    "fp2 = pd.read_csv(\"/Users/nooralindeflaten/f1_ML_predictor/data/processed/leclerc_2022_monaco_fp2.csv\")\n",
    "fp3 = pd.read_csv(\"/Users/nooralindeflaten/f1_ML_predictor/data/processed/leclerc_2022_monaco_fp3.csv\")\n",
    "\n",
    "# Add session label for clarity\n",
    "fp1[\"Session\"] = \"FP1\"\n",
    "fp2[\"Session\"] = \"FP2\"\n",
    "fp3[\"Session\"] = \"FP3\"\n",
    "\n",
    "# Combine them\n",
    "practice_df = pd.concat([fp1, fp2, fp3], ignore_index=True)\n",
    "\n",
    "# Select relevant columns\n",
    "degradation_df = practice_df[[\"LapNumber\",\n",
    "    \"LapTime\", \"TyreLife\", \"Compound\", \"TrackTemp\", \"Pressure\",\n",
    "    \"AirTemp\", \"Humidity\", \"Driver\", \"Session\"\n",
    "]]\n",
    "\n",
    "# Optional: convert LapTime to total seconds if not already\n",
    "degradation_df[\"LapTime\"] = pd.to_timedelta(degradation_df[\"LapTime\"]).dt.total_seconds()\n",
    "\n",
    "# Drop any missing or nonsense values\n",
    "degradation_df = degradation_df.dropna(subset=[\"LapTime\", \"TyreLife\", \"Compound\"])\n",
    "degradation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize degradation by compound\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=practice_df, x=\"TyreLife\", y=\"LapTime\", hue=\"Compound\", errorbar=None)\n",
    "plt.title(\"Lap Time vs. Tyre Life by Compound\")\n",
    "plt.xlabel(\"Tyre Life (laps)\")\n",
    "plt.ylabel(\"Lap Time (s)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = degradation_df[[\"TyreLife\", \"Compound\", \"TrackTemp\", \"AirTemp\", \"Pressure\"]]\n",
    "y = degradation_df[\"LapTime\"]\n",
    "\n",
    "# Encode categorical 'Compound'\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"compound\", OneHotEncoder(drop=\"first\"), [\"Compound\"])\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Pipeline: Encoding + Regression\n",
    "model = make_pipeline(preprocessor, LinearRegression())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Check coefficients (interpretation time!)\n",
    "linreg = model.named_steps[\"linearregression\"]\n",
    "features = model.named_steps[\"columntransformer\"].get_feature_names_out()\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": linreg.coef_\n",
    "})\n",
    "\n",
    "compounds = ['SOFT', 'MEDIUM', 'HARD']\n",
    "tyrelife = list(range(1, 21))  # Simulate laps 1 to 20\n",
    "track_temp = 54.2\n",
    "air_temp = 30.8\n",
    "pressure = 1014.8\n",
    "\n",
    "data = []\n",
    "for compound in compounds:\n",
    "    for tl in tyrelife:\n",
    "        features = {\n",
    "            \"Compound\": compound,\n",
    "            \"TyreLife\": tl,\n",
    "            \"TrackTemp\": track_temp,\n",
    "            \"AirTemp\": air_temp,\n",
    "            \"Pressure\": pressure\n",
    "        }\n",
    "        data.append(features)\n",
    "\n",
    "input_df = pd.DataFrame(data)\n",
    "# Print out the feature names expected by the model\n",
    "print(input_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def simulate_stint(model, compound, track_temp, air_temp, pressure, laps=20):\n",
    "    # One-hot encoding for compound\n",
    "    compounds = ['SOFT', 'MEDIUM', 'HARD']\n",
    "    compound_onehot = {f'compound__{c}': 0 for c in compounds}\n",
    "    compound_onehot[f'compound__{compound}'] = 1\n",
    "    \n",
    "    compounds = ['SOFT', 'MEDIUM', 'HARD']\n",
    "    tyrelife = list(range(1, 21))  # Simulate laps 1 to 20\n",
    "    track_temp = 54.2\n",
    "    air_temp = 30.8\n",
    "    pressure = 1014.8\n",
    "\n",
    "    data = []\n",
    "    for compound in compounds:\n",
    "        for tl in tyrelife:\n",
    "            features = {\n",
    "                \"Compound\": compound,\n",
    "                \"TyreLife\": tl,\n",
    "                \"TrackTemp\": track_temp,\n",
    "                \"AirTemp\": air_temp,\n",
    "                \"Pressure\": pressure\n",
    "            }\n",
    "            data.append(features)\n",
    "\n",
    "    input_df = pd.DataFrame(data)\n",
    "    return input_df\n",
    "# Print out the feature names expected by the model\n",
    "\n",
    "# Simulating stints for different compounds\n",
    "soft_stint = simulate_stint(model, 'SOFT', 54.155, 30.845, 1014.821)\n",
    "medium_stint = simulate_stint(model, 'MEDIUM', 54.155, 30.845, 1014.821)\n",
    "hard_stint = simulate_stint(model, 'HARD', 54.155, 30.845, 1014.821)\n",
    "\n",
    "# Combine stints into a single DataFrame\n",
    "stints_df = pd.concat([soft_stint, medium_stint, hard_stint], ignore_index=True)\n",
    "\n",
    "# Display the result (or plot if needed)\n",
    "print(stints_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_stint(model, compound, track_temp, air_temp, pressure, laps=20):\n",
    "    # One-hot encoding for compound\n",
    "    compounds = ['SOFT', 'MEDIUM', 'HARD']\n",
    "    compound_onehot = {f'compound__{c}': 0 for c in compounds}\n",
    "    compound_onehot[f'compound__{compound}'] = 1\n",
    "    \n",
    "    tyrelife = list(range(1, laps + 1))  # Simulate laps 1 to 20\n",
    "\n",
    "    data = []\n",
    "    for tl in tyrelife:\n",
    "        features = {\n",
    "            \"Compound\": compound,\n",
    "            \"TyreLife\": tl,\n",
    "            \"TrackTemp\": track_temp,\n",
    "            \"AirTemp\": air_temp,\n",
    "            \"Pressure\": pressure,\n",
    "            **compound_onehot  # Add the one-hot encoded compound data\n",
    "        }\n",
    "        data.append(features)\n",
    "\n",
    "    input_df = pd.DataFrame(data)\n",
    "\n",
    "    # Ensure the input columns are in the same order as the model expects\n",
    "    input_df = input_df[model.feature_names_in_]\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(input_df)\n",
    "\n",
    "    # Return result DataFrame with predictions\n",
    "    return pd.DataFrame({\n",
    "        'LapNumber': range(1, laps + 1),\n",
    "        'PredictedLapTime': predictions,\n",
    "        'Compound': compound\n",
    "    })\n",
    "\n",
    "# Simulating stints for different compounds\n",
    "soft_stint = simulate_stint(model, 'SOFT', 54.155, 30.845, 1014.821)\n",
    "medium_stint = simulate_stint(model, 'MEDIUM', 54.155, 30.845, 1014.821)\n",
    "hard_stint = simulate_stint(model, 'HARD', 54.155, 30.845, 1014.821)\n",
    "\n",
    "# Combine stints into a single DataFrame\n",
    "stints_df = pd.concat([soft_stint, medium_stint, hard_stint], ignore_index=True)\n",
    "\n",
    "# Display the result\n",
    "print(stints_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_session_df(driver_laps, session_name=\"Session\"):\n",
    "    import pandas as pd\n",
    "    \n",
    "    driver_laps['LapStartTime'] = pd.to_timedelta(driver_laps['LapStartTime'])\n",
    "    aggregates = []\n",
    "\n",
    "    for _, lap in driver_laps.iterlaps():\n",
    "        try:\n",
    "            car_data = lap.get_car_data().add_distance()\n",
    "            telemetry = {\n",
    "                'LapNumber': lap['LapNumber'],\n",
    "                'speed_avg': car_data['Speed'].mean(),\n",
    "                'rpm_avg': car_data['RPM'].mean(),\n",
    "                'throttle_avg': car_data['Throttle'].mean(),\n",
    "                'brake_avg': car_data['Brake'].mean(),\n",
    "            }\n",
    "\n",
    "            # Weather data for this lap (if available)\n",
    "            weather_data = lap.get_weather_data()\n",
    "            if weather_data is not None:\n",
    "                telemetry.update({\n",
    "                    'TrackTemp': weather_data['TrackTemp'],\n",
    "                    'AirTemp': weather_data['AirTemp'],\n",
    "                    'Pressure': weather_data['Pressure'],\n",
    "                    'Humidity': weather_data['Humidity'],\n",
    "                    'Rainfall': weather_data['Rainfall'],\n",
    "                    'WindSpeed': weather_data['WindSpeed'],\n",
    "                    'WindDirection': weather_data['WindDirection'],\n",
    "                })\n",
    "\n",
    "            telemetry['Session'] = session_name\n",
    "            aggregates.append(telemetry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping lap {lap.LapNumber} due to error: {e}\")\n",
    "\n",
    "    agg_df = pd.DataFrame(aggregates)\n",
    "    driver_laps = driver_laps.copy()\n",
    "    driver_laps['LapTimeSec'] = pd.to_timedelta(driver_laps['LapTime']).dt.total_seconds()\n",
    "    return driver_laps.merge(agg_df, on=\"LapNumber\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_df = build_session_df(leclerc_race_laps, \"Race\")\n",
    "quali_df = build_session_df(leclerc_quali_laps, \"Quali\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fast_laps(df, percentile=0.7):\n",
    "    threshold = df['LapTimeSec'].quantile(percentile)\n",
    "    return df[df['LapTimeSec'] <= threshold].reset_index(drop=True)\n",
    "\n",
    "def get_slow_laps(df, percentile=0.3):\n",
    "    threshold = df['LapTimeSec'].quantile(percentile)\n",
    "    return df[df['LapTimeSec'] >= threshold].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_laps = get_fast_laps(race_df)\n",
    "slow_laps = get_slow_laps(race_df)\n",
    "\n",
    "print(f\"Fast laps count: {len(fast_laps)}\")\n",
    "print(f\"Slow laps count: {len(slow_laps)}\")\n",
    "\n",
    "# Quick comparison\n",
    "cols_to_compare = ['speed_avg', 'throttle_avg', 'brake_avg', 'TrackTemp', 'Rainfall', 'TyreLife']\n",
    "\n",
    "print(\"Fast Lap Averages:\")\n",
    "print(fast_laps[cols_to_compare].mean())\n",
    "\n",
    "print(\"\\nSlow Lap Averages:\")\n",
    "print(slow_laps[cols_to_compare].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def preprocess_lap_data_for_clustering(df,n_clusters=4):\n",
    "    # Convert LapTime and SectorTimes to total seconds\n",
    "    time_cols = ['LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time']\n",
    "    for col in time_cols:\n",
    "        df[col] = pd.to_timedelta(df[col]).dt.total_seconds()\n",
    "    \n",
    "    # Select relevant numerical features\n",
    "    features = [\n",
    "        'LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time',\n",
    "        'speed_avg', 'rpm_avg', 'throttle_avg', 'brake_avg',\n",
    "        'TrackTemp', 'Rainfall', 'TyreLife', 'Pressure',\n",
    "        'AirTemp', 'Humidity', 'WindSpeed'\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    # Drop rows with missing values in selected features\n",
    "    cluster_data = df.dropna(subset=features).copy()\n",
    "    indices = cluster_data.index\n",
    "\n",
    "    # Step 5: Scale the features\n",
    "    X = cluster_data[features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Step 6: Run KMeans clustering\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    labels = model.fit_predict(X_scaled)\n",
    "\n",
    "    # Step 7: Attach labels back to the original DataFrame\n",
    "    df.loc[indices, 'cluster'] = labels # return the index so we can attach cluster labels back later\n",
    "    return df, model\n",
    "\n",
    "def run_kmeans_clustering(X_scaled, n_clusters=4):\n",
    "    # Fit KMeans model\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    return cluster_labels, kmeans\n",
    "\n",
    "\"Clustering functions defined and ready to use!\"\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def combine_sessions_and_cluster(fp1_df,fp2_df,fp3_df, quali_laps, race_laps, n_clusters=4):\n",
    "    # Step 1: Combine all laps into one DataFrame\n",
    "    combined_df = pd.concat([fp1_df,fp2_df,fp3_df, quali_laps, race_laps], ignore_index=True)\n",
    "    \n",
    "    # Step 2: Convert time features to total seconds (so theyâ€™re numeric)\n",
    "    for col in ['LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time']:\n",
    "        combined_df[col] = combined_df[col].dt.total_seconds()\n",
    "\n",
    "    # Step 3: Define features for clustering\n",
    "    features = [\n",
    "        'LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time',\n",
    "        'speed_avg', 'rpm_avg', 'throttle_avg', 'brake_avg',\n",
    "        'TyreLife', 'TrackTemp', 'Pressure', 'AirTemp', 'Humidity',\n",
    "        'WindSpeed', 'Rainfall'\n",
    "    ]\n",
    "\n",
    "    # Step 4: Drop NaNs and keep a reference to their original index\n",
    "    cluster_data = combined_df.dropna(subset=features).copy()\n",
    "    indices = cluster_data.index\n",
    "\n",
    "    # Step 5: Scale the features\n",
    "    X = cluster_data[features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Step 6: Run KMeans clustering\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    labels = model.fit_predict(X_scaled)\n",
    "\n",
    "    # Step 7: Attach labels back to the original DataFrame\n",
    "    combined_df.loc[indices, 'cluster'] = labels\n",
    "\n",
    "    return combined_df, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_df, cluster_model = combine_sessions_and_cluster(fp1,fp2,fp3, quali_df, race_df, n_clusters=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_clusters(clustered_df, group_col='cluster'):\n",
    "    summary = clustered_df.groupby(group_col)[[\n",
    "        'LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time',\n",
    "        'speed_avg', 'rpm_avg', 'throttle_avg', 'brake_avg',\n",
    "        'TyreLife', 'TrackTemp', 'Rainfall', 'AirTemp', 'Humidity'\n",
    "    ]].mean().sort_index()\n",
    "\n",
    "    count = clustered_df[group_col].value_counts().sort_index().rename('Lap Count')\n",
    "    \n",
    "    return summary.join(count)\n",
    "cluster_summary = summarize_clusters(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_cluster, indices = preprocess_lap_data_for_clustering(fp1,n_clusters=4)\n",
    "print(fp_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_samples(df, n_samples=3, random_state=42):\n",
    "    \"\"\"Return n sample laps from each cluster in the DataFrame.\"\"\"\n",
    "    samples = df.groupby('cluster').apply(\n",
    "        lambda x: x.sample(min(len(x), n_samples), random_state=random_state)\n",
    "    )\n",
    "    # Remove multi-index from groupby\n",
    "    samples.reset_index(drop=True, inplace=True)\n",
    "    return samples\n",
    "\n",
    "samples_fp1 = get_cluster_samples(fp1, n_samples=3)\n",
    "samples_fp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corner_segments(car,pos,circuit_info, margin=10):\n",
    "    from scipy.spatial import cKDTree\n",
    "\n",
    "    segments = {}\n",
    "    # Create a KDTree for fast spatial lookup\n",
    "    track_coords = pos[['X', 'Y']].to_numpy()\n",
    "    dist_tree = cKDTree(track_coords)\n",
    "\n",
    "    for _, corner in circuit_info.corners.iterrows():\n",
    "        # Corner name\n",
    "        name = f\"{corner['Number']}{corner['Letter']}\"\n",
    "\n",
    "        # Find closest telemetry point to corner coordinate\n",
    "        corner_coord = [corner['X'], corner['Y']]\n",
    "        dist, idx = dist_tree.query(corner_coord)\n",
    "\n",
    "        # Get the distance at that point in telemetry\n",
    "        try:\n",
    "            center_dist = car.iloc[idx]['Distance']\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        # Slice Â±margin meters around that distance\n",
    "        mask = (car['Distance'] >= center_dist - margin) & (car['Distance'] <= center_dist + margin)\n",
    "        segment = car[mask].copy()\n",
    "        segments[name] = segment\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_car_and_pos(car_data, pos_data):\n",
    "    pos_data_interp = pos_data.set_index('SessionTime')[['X', 'Y']].interpolate(method='time')\n",
    "    merged = car_data.set_index('SessionTime').join(pos_data_interp, how='left')\n",
    "    return merged.reset_index()\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "\n",
    "def label_corners_in_pos(pos_data, corner_coords, radius=15):\n",
    "    # Build KDTree for corners\n",
    "    corner_points = np.array([[c['x'], c['y']] for c in corner_coords])\n",
    "    tree = cKDTree(corner_points)\n",
    "    \n",
    "    # For each position point, find nearest corner (if close enough)\n",
    "    coords = pos_data[['X', 'Y']].values\n",
    "    distances, indices = tree.query(coords, distance_upper_bound=radius)\n",
    "    \n",
    "    # Label with corner names (NaN if too far)\n",
    "    corner_labels = [\n",
    "        corner_coords[i]['name'] if d < radius else np.nan\n",
    "        for i, d in zip(indices, distances)\n",
    "    ]\n",
    "    \n",
    "    pos_data['corner'] = corner_labels\n",
    "    return pos_data\n",
    "\n",
    "def interpolate_corner_labels(car_data, pos_data_with_corners):\n",
    "    pos_data_labeled = pos_data_with_corners.set_index('SessionTime')[['corner']]\n",
    "    \n",
    "    # Resample to fill gaps and align to car_data time range\n",
    "    pos_data_resampled = pos_data_labeled.resample('10ms').ffill()\n",
    "\n",
    "    # Merge using SessionTime index\n",
    "    car_data_labeled = car_data.set_index('SessionTime').join(pos_data_resampled, how='left')\n",
    "    return car_data_labeled.reset_index()\n",
    "\n",
    "def summarize_driver_behavior_by_corner(car_data_labeled):\n",
    "    corner_groups = car_data_labeled.dropna(subset=['corner']).groupby('corner')\n",
    "    summary = corner_groups[['Throttle', 'Brake', 'Speed', 'RPM']].mean()\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full context\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "\n",
    "def label_pos_data_with_corners(pos_data, corner_coords, radius=15):\n",
    "    # KDTree setup\n",
    "    corner_points = np.array([[c['x'], c['y']] for c in corner_coords])\n",
    "    tree = cKDTree(corner_points)\n",
    "\n",
    "    # Find nearest corner for each position\n",
    "    pos_points = pos_data[['X', 'Y']].values\n",
    "    distances, indices = tree.query(pos_points, distance_upper_bound=radius)\n",
    "\n",
    "    # Assign corner names (or NaN if too far)\n",
    "    pos_data['corner'] = [\n",
    "        corner_coords[i]['name'] if d < radius else np.nan\n",
    "        for i, d in zip(indices, distances)\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(pos_data)\n",
    "\n",
    "def merge_corner_labels_into_car_data(car_data, pos_data_with_corners, resample_rate=''):\n",
    "    # Prepare pos_data with just SessionTime and corner info\n",
    "    labeled_pos = pos_data_with_corners[['SessionTime', 'corner']].dropna().copy()\n",
    "    labeled_pos.set_index('SessionTime', inplace=True)\n",
    "    labeled_pos = labeled_pos[~labeled_pos.index.duplicated(keep='first')]  # avoid duplicates\n",
    "    \n",
    "    # Resample to match car_data granularity\n",
    "    pos_resampled = labeled_pos.resample(resample_rate).ffill()\n",
    "\n",
    "    # Merge corner labels into car_data\n",
    "    car_data_labeled = car_data.set_index('SessionTime').join(pos_resampled, how='left')\n",
    "\n",
    "    return pd.DataFrame(car_data_labeled.reset_index())\n",
    "\n",
    "fp1 = all_laps_by_session['FP1']\n",
    "fp2 = all_laps_by_session['FP2']\n",
    "fp3 = all_laps_by_session['FP3']\n",
    "\n",
    "track = fp1.session.get_circuit_info()\n",
    "lap = fp1.pick_lap(5)\n",
    "car_data = lap.get_car_data().add_distance()\n",
    "pos_data = lap.get_pos_data()\n",
    "corners = []\n",
    "\n",
    "for _, row in track.corners.iterrows():\n",
    "    corners.append({\n",
    "        'name': f\"{row['Number']}{row['Letter']}\",\n",
    "        'x': row['X'],\n",
    "        'y': row['Y']\n",
    "    })\n",
    "\n",
    "\n",
    "# Label pos_data\n",
    "pos_labeled = label_pos_data_with_corners(pos_data, corners)\n",
    "\n",
    "# Add corner info to car_data\n",
    "car_labeled = merge_corner_labels_into_car_data(car_data, pos_labeled)\n",
    "\n",
    "# Now you can easily do:\n",
    "car_labeled[car_labeled['corner'] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_cluster_radar(df_summary, features, cluster_ids=None):\n",
    "    if cluster_ids is None:\n",
    "        cluster_ids = df_summary.index.tolist()\n",
    "\n",
    "    num_vars = len(features)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "    for cluster in cluster_ids:\n",
    "        values = df_summary.loc[cluster, features].tolist()\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, label=f'Cluster {int(cluster)}')\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "    ax.set_theta_offset(np.pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "    ax.set_thetagrids(np.degrees(angles[:-1]), features)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "    plt.title(\"Driving Style Cluster Profiles\", fontsize=15)\n",
    "    plt.show()\n",
    "\n",
    "# Use this:\n",
    "features_to_plot = ['speed_avg', 'rpm_avg', 'throttle_avg', 'brake_avg', 'TyreLife', 'TrackTemp']\n",
    "plot_cluster_radar(cluster_summary, features=features_to_plot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cluster_laps(df, n_clusters=3, features=['speed_avg', 'throttle_avg', 'brake_avg']):\n",
    "    \"\"\"\n",
    "    Cluster laps based on selected driving behavior features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Lap summary DataFrame.\n",
    "        n_clusters (int): Number of clusters to create.\n",
    "        features (list): Columns to use for clustering.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Original DataFrame with 'cluster' column added.\n",
    "    \"\"\"\n",
    "    # Drop rows with missing feature data\n",
    "    cluster_df = df.dropna(subset=features).copy()\n",
    "\n",
    "    # Normalize\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(cluster_df[features])\n",
    "\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Add cluster labels\n",
    "    cluster_df['cluster'] = cluster_labels\n",
    "\n",
    "    return cluster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sectors_by_cluster(df):\n",
    "    \"\"\"\n",
    "    Prints average sector times per cluster.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Sector Comparison by Cluster ===\")\n",
    "    for cluster in sorted(df['cluster'].unique()):\n",
    "        cluster_data = df[df['cluster'] == cluster]\n",
    "        print(f\"\\nCluster {cluster} ({len(cluster_data)} laps):\")\n",
    "        print(cluster_data[['sector1', 'sector2', 'sector3']].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_laps = [fp1_df,fp2_df,fp3_df,quali,races]\n",
    "lap_summary = cluster_laps(all_laps, n_clusters=3)\n",
    "compare_sectors_by_cluster(lap_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF273",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
